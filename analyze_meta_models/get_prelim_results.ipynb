{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from random import sample, choice\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes kFold cross validation (10) and it grabs the max(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### only needs to be run once ####\n",
    "def multiple_max(df):\n",
    "    df1 = df.drop('dataset', axis=1)\n",
    "\n",
    "    maxes = {}      # {dataset: [max1, max2, ...]}\n",
    "    max_val = {}    # {dataset: max_val}\n",
    "    for i in range(df.shape[0]):\n",
    "        row = np.array(df1.iloc[i])\n",
    "        row_maxes = np.argwhere(row == np.amax(row)).flatten().tolist()\n",
    "        max_val[df.iloc[i, 0]] = np.amax(row)\n",
    "        maxes[df.iloc[i, 0]] = row_maxes\n",
    "        # maxes[df.iloc[i, 0]] = df1.columns[row_maxes].tolist() # if you want col names\n",
    "    return maxes, max_val\n",
    "\n",
    "pre = pd.read_csv(\"emp_results.csv\")\n",
    "pre['dataset'] = pre['dataset'].apply(lambda x: int(re.split(r'\\.arff', x)[0]))\n",
    "\n",
    "optimized_cols = []\n",
    "default_cols = []\n",
    "\n",
    "for col in pre.columns:\n",
    "    if col == 'dataset':\n",
    "        optimized_cols.append(col)\n",
    "        default_cols.append(col)\n",
    "    if \"+\" in col:\n",
    "        optimized_cols.append(col)\n",
    "    elif \"-\" in col:\n",
    "        default_cols.append(col)\n",
    "\n",
    "optim_maxes, optim_max_values = multiple_max(pre[optimized_cols])\n",
    "default_maxes, default_max_values = multiple_max(pre[default_cols])\n",
    "\n",
    "# optim_maxes = {dataset: [max1, max2, ..., max466]}\n",
    "# default_maxes = {dataset: [max1, max2, ..., max466]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo train_meta_model but with 2 fold cross validation\n",
    "def train_meta_model(sk_algorithm, X, y, valid_datasets, maxes):\n",
    "\taccuracies = []\n",
    "\t# for i in range(iters):\n",
    "\t# proably want to shuffle the data before running kfold \n",
    "\t# to do this, need to combine X, y, and valid_datasets into one dataframe\n",
    "\t# I think we want to combine valid_dataset, becuase it is the key? \n",
    "\tkf = KFold(n_splits=10)\n",
    "\tkf.get_n_splits(X)\n",
    "\tfor train_index, test_index in kf.split(X):\n",
    "\t\tX_train, X_test = X[train_index], X[test_index]\n",
    "\t\ty_train, y_test = y[train_index], y[test_index] # dont use y_test\n",
    "\t\tds_train, ds_test = valid_datasets[train_index], valid_datasets[test_index] # valid_datasets is the list of datasets\n",
    "\t\tmodel = sk_algorithm\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\ty_pred = model.predict(X_test)\n",
    "\t\tcorrect = 0\n",
    "\t\tfor ds, prediction in zip(ds_test, y_pred):\n",
    "\t\t\tif prediction in maxes[ds]:\n",
    "\t\t\t\tcorrect += 1\n",
    "\t\taccuracies.append(correct/len(ds_test))\n",
    "\treturn max(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for meta-model\n",
    "models = [\"Metadataset\", \"RandomForest\", \"LogisticRegression\", \"SVC\", \"KNeighbors\",\n",
    "          \"GaussianNB\", \"DecisionTree\", \"AdaBoost\", \"GradientBoosting\",\n",
    "          \"MLP\", \"Bagging\", \"ExtraTrees\",  \"Voting\"]\n",
    "results = []\n",
    "maxes = optim_maxes #optim_maxes_sd\n",
    "iters = 5 # not using this anymore \n",
    "\n",
    "meta_datasets_path = \"meta_datasets\"\n",
    "meta_datasets = {}\n",
    "for meta_dataset in os.listdir(meta_datasets_path):\n",
    "    meta_datasets[meta_dataset] = pd.read_csv(os.path.join(meta_datasets_path, meta_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: concept_metadataset.csv\n",
      "177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:   8%|▊         | 1/13 [00:17<03:35, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: clustering_metadataset.csv\n",
      "428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  15%|█▌        | 2/13 [00:41<03:55, 21.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: all_metadataset.csv\n",
      "399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  23%|██▎       | 3/13 [02:33<10:24, 62.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: general_metadataset.csv\n",
      "437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  31%|███       | 4/13 [02:54<06:56, 46.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: model-based_metadataset.csv\n",
      "391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  38%|███▊      | 5/13 [03:26<05:28, 41.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: landmarking_metadataset.csv\n",
      "434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  46%|████▌     | 6/13 [03:55<04:20, 37.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: itemset_metadataset.csv\n",
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  54%|█████▍    | 7/13 [04:15<03:08, 31.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: complexity_metadataset.csv\n",
      "403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  62%|██████▏   | 8/13 [04:51<02:43, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: info-theory_metadataset.csv\n",
      "437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  69%|██████▉   | 9/13 [05:18<02:04, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: d2v_pretrained_metadaset.csv\n",
      "437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  77%|███████▋  | 10/13 [05:58<01:41, 33.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: default_metadataset.csv\n",
      "434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  85%|████████▍ | 11/13 [07:23<01:38, 49.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: statistical_metadataset.csv\n",
      "437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets:  92%|█████████▏| 12/13 [08:09<00:48, 48.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name: relative_metadataset.csv\n",
      "434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-datasets: 100%|██████████| 13/13 [08:30<00:00, 39.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in tqdm(meta_datasets.items(), desc=\"Meta-datasets\"):\n",
    "    # df_name, df = choice(list(meta_datasets.items()))\n",
    "    print('df_name:', df_name)\n",
    "    results_arr = [df_name]\n",
    "    X = []\n",
    "    y = []\n",
    "    valid_datasets = []\n",
    "\n",
    "    # df_name, df = choice(list(meta_datasets.items()))\n",
    "    df.replace(np.nan, 0, inplace=True)\n",
    "    df.replace(np.inf, 0, inplace=True)\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        if df.loc[i, 'dataset'] in maxes:\n",
    "            y.append(sample(maxes[df.loc[i, 'dataset']],k=1)[0])\n",
    "            X.append(df.iloc[i, 1:].tolist())\n",
    "            valid_datasets.append(df.loc[i, 'dataset'])\n",
    "    print(len(valid_datasets))\n",
    "    X, y, valid_datasets = np.array(X), np.array(y), np.array(valid_datasets)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "    rf_accuracies = train_meta_model(rf, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(rf_accuracies))\n",
    "    # print(f'rf accuracy: {round(np.mean(rf_accuracies)*100,2)}%', flush=True)\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "    lr_accuracies = train_meta_model(lr, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(lr_accuracies))\n",
    "    # print(f'lr accuracy: {round(np.mean(lr_accuracies)*100,2)}%', flush=True)\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC(gamma='auto')\n",
    "    svm_accuracies = train_meta_model(svm, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(svm_accuracies))\n",
    "    # print(f'svm accuracy: {round(np.mean(svm_accuracies)*100,2)}%', flush=True)\n",
    "\n",
    "    # KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_accuracies = train_meta_model(knn, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(knn_accuracies))\n",
    "    # print(f'knn accuracy: {round(np.mean(knn_accuracies)*100,2)}%', flush=True)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb_accuracies = train_meta_model(nb, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(nb_accuracies))\n",
    "    # print(f'nb accuracy: {round(np.mean(nb_accuracies)*100,2)}%')\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(random_state=0)\n",
    "    dt_accuracies = train_meta_model(dt, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(dt_accuracies))\n",
    "    # print(f'dt accuracy: {round(np.mean(dt_accuracies)*100,2)}%')\n",
    "\n",
    "    # AdaBoost\n",
    "    ab = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    ab_accuracies = train_meta_model(ab, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(ab_accuracies))\n",
    "    # print(f'ab accuracy: {round(np.mean(ab_accuracies)*100,2)}%')\n",
    "\n",
    "    # Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    gb_accuracies = train_meta_model(gb, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(gb_accuracies))\n",
    "    # print(f'gb accuracy: {round(np.mean(gb_accuracies)*100,2)}%')\n",
    "\n",
    "    # Neural Network\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    nn_accuracies = train_meta_model(nn, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(nn_accuracies))\n",
    "    # print(f'nn accuracy: {round(np.mean(nn_accuracies)*100,2)}%')\n",
    "\n",
    "    # Bagging\n",
    "    bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=0)\n",
    "    bg_accuracies = train_meta_model(bg, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(bg_accuracies))\n",
    "    # print(f'bg accuracy: {round(np.mean(bg_accuracies)*100,2)}%')\n",
    "\n",
    "    # Extra Trees\n",
    "    et = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    et_accuracies = train_meta_model(et, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(et_accuracies))\n",
    "    # print(f'et accuracy: {round(np.mean(et_accuracies)*100,2)}%')\n",
    "\n",
    "    # Voting Classifier\n",
    "    vc = VotingClassifier(estimators=[('rf', rf), ('lr', lr), ('svm', svm), ('knn', knn), ('nb', nb), ('dt', dt), ('ab', ab), ('gb', gb), ('nn', nn), ('bg', bg), ('et', et)], voting='hard')\n",
    "    vc_accuracies = train_meta_model(vc, X, y, valid_datasets, maxes)\n",
    "    results_arr.append(np.mean(vc_accuracies))\n",
    "    # print(f'vc accuracy: {round(np.mean(vc_accuracies)*100,2)}%')\n",
    "\n",
    "    results.append(results_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metadataset</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>SVC</th>\n",
       "      <th>KNeighbors</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Bagging</th>\n",
       "      <th>ExtraTrees</th>\n",
       "      <th>Voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concept_metadataset.csv</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clustering_metadataset.csv</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.627907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_metadataset.csv</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general_metadataset.csv</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model-based_metadataset.csv</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>landmarking_metadataset.csv</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>itemset_metadataset.csv</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>complexity_metadataset.csv</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>info-theory_metadataset.csv</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d2v_pretrained_metadaset.csv</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>default_metadataset.csv</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>statistical_metadataset.csv</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relative_metadataset.csv</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Metadataset  RandomForest  LogisticRegression       SVC   \n",
       "0        concept_metadataset.csv      0.555556            0.611111  0.555556  \\\n",
       "1     clustering_metadataset.csv      0.511628            0.558140  0.604651   \n",
       "2            all_metadataset.csv      0.775000            0.450000  0.575000   \n",
       "3        general_metadataset.csv      0.704545            0.590909  0.568182   \n",
       "4    model-based_metadataset.csv      0.525000            0.358974  0.625000   \n",
       "5    landmarking_metadataset.csv      0.581395            0.534884  0.534884   \n",
       "6        itemset_metadataset.csv      0.522727            0.522727  0.545455   \n",
       "7     complexity_metadataset.csv      0.725000            0.350000  0.575000   \n",
       "8    info-theory_metadataset.csv      0.545455            0.545455  0.568182   \n",
       "9   d2v_pretrained_metadaset.csv      0.522727            0.568182  0.545455   \n",
       "10       default_metadataset.csv      0.604651            0.372093  0.545455   \n",
       "11   statistical_metadataset.csv      0.522727            0.363636  0.545455   \n",
       "12      relative_metadataset.csv      0.604651            0.604651  0.604651   \n",
       "\n",
       "    KNeighbors  GaussianNB  DecisionTree  AdaBoost  GradientBoosting   \n",
       "0     0.555556    0.444444      0.444444  0.500000          0.352941  \\\n",
       "1     0.441860    0.325581      0.441860  0.465116          0.627907   \n",
       "2     0.400000    0.400000      0.475000  0.575000          0.425000   \n",
       "3     0.454545    0.318182      0.590909  0.386364          0.590909   \n",
       "4     0.435897    0.435897      0.512821  0.461538          0.525000   \n",
       "5     0.651163    0.511628      0.604651  0.488372          0.674419   \n",
       "6     0.395349    0.522727      0.500000  0.418605          0.454545   \n",
       "7     0.550000    0.275000      0.650000  0.650000          0.600000   \n",
       "8     0.348837    0.363636      0.454545  0.522727          0.386364   \n",
       "9     0.431818    0.295455      0.431818  0.386364          0.477273   \n",
       "10    0.386364    0.372093      0.441860  0.697674          0.720930   \n",
       "11    0.590909    0.340909      0.500000  0.522727          0.613636   \n",
       "12    0.409091    0.545455      0.441860  0.568182          0.511628   \n",
       "\n",
       "         MLP   Bagging  ExtraTrees    Voting  \n",
       "0   0.500000  0.500000    0.444444  0.555556  \n",
       "1   0.511628  0.511628    0.651163  0.627907  \n",
       "2   0.275000  0.650000    0.725000  0.750000  \n",
       "3   0.545455  0.590909    0.568182  0.636364  \n",
       "4   0.307692  0.461538    0.487179  0.512821  \n",
       "5   0.500000  0.581395    0.651163  0.604651  \n",
       "6   0.545455  0.454545    0.454545  0.545455  \n",
       "7   0.575000  0.675000    0.725000  0.775000  \n",
       "8   0.545455  0.522727    0.590909  0.568182  \n",
       "9   0.227273  0.386364    0.395349  0.613636  \n",
       "10  0.227273  0.581395    0.720930  0.674419  \n",
       "11  0.340909  0.500000    0.545455  0.590909  \n",
       "12  0.534884  0.441860    0.581395  0.674419  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"Metadataset\", \"RandomForest\", \"LogisticRegression\", \"SVC\", \"KNeighbors\",\n",
    "          \"GaussianNB\", \"DecisionTree\", \"AdaBoost\", \"GradientBoosting\",\n",
    "          \"MLP\", \"Bagging\", \"ExtraTrees\",  \"Voting\"]\n",
    "\n",
    "df = pd.DataFrame(results, columns=models)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Metadataset\", \"RandomForest\", \"LogisticRegression\", \"SVC\", \"KNeighbors\",\n",
    "          \"GaussianNB\", \"DecisionTree\", \"AdaBoost\", \"GradientBoosting\",\n",
    "          \"MLP\", \"Bagging\", \"ExtraTrees\",  \"Voting\"]\n",
    "\n",
    "df = pd.DataFrame(results, columns=models)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('results.csv')\n",
    "# df.to_csv('results_sd.csv', index=False)\n",
    "# df.to_csv('results.csv', index=False)\n",
    "# df.to_csv('results_sd_2.csv', index=False)\n",
    "# df.to_csv('results_cross_fold.csv', index=False)\n",
    "\n",
    "df.to_csv('results_some_changes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
